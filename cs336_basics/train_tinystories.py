# cs336_basics/train_tinystories_fixed.py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import logging
from pathlib import Path
import sys
import os
import inspect

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

# å¯¼å…¥ä½ å·²æœ‰çš„ä»£ç 
try:
    from cs336_basics.transformer_lm import TransformerLM
    from cs336_basics.AdamW import get_adamw_cls
    from cs336_basics.learning_rate_schedule import get_lr_cosine_schedule
    from cs336_basics.get_batch import get_batch
    from cs336_basics.checkpoint import save_checkpoint, load_checkpoint
    from cs336_basics.cross_entropy import cross_entropy
    print("âœ… æˆåŠŸå¯¼å…¥å·²æœ‰ç»„ä»¶")
    
    # æ£€æŸ¥TransformerLMçš„å‚æ•°
    sig = inspect.signature(TransformerLM.__init__)
    print(f"TransformerLMå‚æ•°: {list(sig.parameters.keys())}")
    
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

class CS336Trainer:
    def __init__(self, config):
        self.config = config
        self.device = config['device']
        
        # æ£€æŸ¥TransformerLMçš„å‚æ•°å¹¶æ­£ç¡®åˆå§‹åŒ–
        sig = inspect.signature(TransformerLM.__init__)
        init_params = {}
        
        # åªä¼ é€’TransformerLMæ¥å—çš„å‚æ•°
        for param_name in sig.parameters:
            if param_name == 'self':
                continue
            if param_name in config:
                init_params[param_name] = config[param_name]
        
        print(f"åˆå§‹åŒ–TransformerLMå‚æ•°: {init_params}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = TransformerLM(**init_params).to(self.device)
        
        # ä½¿ç”¨ä½ å·²æœ‰çš„ä¼˜åŒ–å™¨
        AdamW = get_adamw_cls()
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=config['learning_rate'],
            betas=(config.get('beta1', 0.9), config.get('beta2', 0.999)),
            eps=config.get('epsilon', 1e-8),
            weight_decay=config.get('weight_decay', 0.01)
        )
        
        # ä½¿ç”¨ä½ å·²æœ‰çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_schedule = get_lr_cosine_schedule(
            max_learning_rate=config['learning_rate'],
            min_learning_rate=config.get('min_learning_rate', config['learning_rate'] * 0.1),
            warmup_iters=config.get('warmup_iters', 1000),
            cosine_cycle_iters=config.get('total_iters', 10000)
        )
        
        self.iteration = 0
        self.train_data = self.load_data()
        
        print(f"âœ… æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def load_data(self):
        """åŠ è½½TinyStoriesæ•°æ®"""
        data_path = self.config.get('train_data_path', 
            '/Users/jiangyanze/Desktop/CS336/Assignment1/data/TinyStoriesV2-GPT4-train.txt')
        
        print(f"ğŸ“– åŠ è½½æ•°æ®: {data_path}")
        
        try:
            # è¯»å–æ–‡æœ¬æ–‡ä»¶
            with open(data_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # ç®€å•çš„å­—ç¬¦çº§æ ‡è®°åŒ–
            vocab = {chr(i): i for i in range(256)}  # ASCIIå­—ç¬¦
            vocab_size = len(vocab)
            
            # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°
            tokens = [vocab.get(c, vocab.get(' ', 32)) for c in text[:1000000]]  # é™åˆ¶å¤§å°
            
            print(f"âœ… åŠ è½½æˆåŠŸ: {len(tokens):,} ä¸ªæ ‡è®°")
            # ç¡®ä¿ä½¿ç”¨int64ç±»å‹
            return np.array(tokens, dtype=np.int64)
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
            return self.create_simulated_data()
    
    def create_simulated_data(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®"""
        vocab_size = self.config['vocab_size']
        seq_length = 1000000
        # ç¡®ä¿ä½¿ç”¨int64ç±»å‹
        return np.random.randint(0, vocab_size, size=seq_length, dtype=np.int64)
    
    def compute_loss(self, logits, targets):
        """ä½¿ç”¨ä½ å·²æœ‰çš„äº¤å‰ç†µå‡½æ•°"""
        return cross_entropy(logits, targets)
    
    def get_batch_fixed(self, data, batch_size=None, context_length=None):
        """ä¿®å¤ç‰ˆæœ¬çš„get_batchï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®"""
        if batch_size is None:
            batch_size = self.config['batch_size']
        if context_length is None:
            context_length = self.config['context_length']
        
        # è®¡ç®—å¯ç”¨çš„èµ·å§‹ä½ç½®
        max_start_idx = len(data) - context_length - 1
        
        # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
        start_indices = np.random.randint(0, max_start_idx, size=batch_size)
        
        # åˆ›å»ºç´¢å¼•çŸ©é˜µç”¨äºå‘é‡åŒ–æå–
        indices = start_indices[:, None] + np.arange(context_length)
        
        # æå–è¾“å…¥åºåˆ—
        inputs = data[indices]  # [batch_size, context_length]
        
        # ç›®æ ‡åºåˆ—æ˜¯è¾“å…¥åºåˆ—å‘åç§»åŠ¨ä¸€ä½
        target_indices = start_indices[:, None] + np.arange(1, context_length + 1)
        targets = data[target_indices]  # [batch_size, context_length]
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        # ç¡®ä¿ä½¿ç”¨torch.int64ç±»å‹
        inputs_tensor = torch.from_numpy(inputs).to(torch.int64).to(self.device)
        targets_tensor = torch.from_numpy(targets).to(torch.int64).to(self.device)
        
        return inputs_tensor, targets_tensor
    
    def train_step(self):
        """å•ä¸ªè®­ç»ƒæ­¥éª¤"""
        self.model.train()
        
        # ä½¿ç”¨ä¿®å¤çš„get_batchå‡½æ•°
        inputs, targets = self.get_batch_fixed(self.train_data)
        
        # å‰å‘ä¼ æ’­
        logits = self.model(inputs)
        loss = self.compute_loss(logits, targets)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        if self.config.get('max_grad_norm', 1.0) > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config['max_grad_norm']
            )
        
        # æ›´æ–°å­¦ä¹ ç‡
        current_lr = self.lr_schedule(self.iteration)
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = current_lr
        
        self.optimizer.step()
        self.iteration += 1
        
        return loss.item(), current_lr
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        with torch.no_grad():
            inputs, targets = self.get_batch_fixed(self.train_data)
            logits = self.model(inputs)
            loss = self.compute_loss(logits, targets)
        return loss.item()
    
    def train(self, num_steps=None):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        total_iters = self.config.get('total_iters', 100)  # å…ˆç”¨100æ­¥æµ‹è¯•
        train_losses = []
        val_losses = []
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {total_iters} æ­¥")
        print("=" * 60)
        
        for step in range(total_iters):
            train_loss, current_lr = self.train_step()
            train_losses.append(train_loss)
            
            # è®°å½•æ—¥å¿—
            if step % 10 == 0 or step == total_iters - 1:
                val_loss = self.evaluate()
                val_losses.append(val_loss)
                
                tokens_processed = (step + 1) * self.config['batch_size'] * self.config['context_length']
                print(f"Step {step:5d} | "
                      f"LR: {current_lr:.2e} | "
                      f"Train: {train_loss:.4f} | "
                      f"Val: {val_loss:.4f} | "
                      f"Tokens: {tokens_processed:,}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if step % 100 == 0 and step > 0:
                checkpoint_path = f"checkpoint_step_{step}.pt"
                save_checkpoint(self.model, self.optimizer, step, checkpoint_path)
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        return train_losses, val_losses

def run_lr_sweep_experiment():
    """è¿è¡Œå­¦ä¹ ç‡æ‰«æå®éªŒ"""
    
    # åŸºç¡€é…ç½®
    base_config = {
        # æ•°æ®é…ç½®
        'train_data_path': '/Users/jiangyanze/Desktop/CS336/Assignment1/data/TinyStoriesV2-GPT4-train.txt',
        'vocab_size': 10000,
        
        # æ¨¡å‹é…ç½® - åªåŒ…å«TransformerLMå®é™…æ¥å—çš„å‚æ•°
        'd_model': 512,
        'num_layers': 6,
        'num_heads': 8,
        'd_ff': 2048,
        'context_length': 256,  # æ³¨æ„ï¼šè¿™æ˜¯TransformerLMéœ€è¦çš„å‚æ•°
        'theta': 10000,  # RoPEçš„thetaå‚æ•°
        
        # è®­ç»ƒé…ç½®
        'batch_size': 32,
        'total_iters': 100,  # å…ˆç”¨100æ­¥æµ‹è¯•
        'max_grad_norm': 1.0,
        
        # ä¼˜åŒ–å™¨é…ç½®
        'beta1': 0.9,
        'beta2': 0.999,
        'epsilon': 1e-8,
        'weight_decay': 0.01,
        
        # ç³»ç»Ÿé…ç½®
        'device': 'cpu',
        'min_learning_rate': 1e-5,
        'warmup_iters': 100
    }
    
    # å­¦ä¹ ç‡æ‰«æèŒƒå›´
    learning_rates = [1e-5, 1e-4, 1e-3]  # å…ˆç”¨å°‘é‡å­¦ä¹ ç‡æµ‹è¯•
    results = {}
    
    print("ğŸ¯ CS336 å­¦ä¹ ç‡æ‰«æå®éªŒ")
    print("=" * 60)
    
    for i, lr in enumerate(learning_rates):
        print(f"\nğŸ”¬ å®éªŒ {i+1}/{len(learning_rates)}: LR = {lr:.2e}")
        print("-" * 40)
        
        config = base_config.copy()
        config['learning_rate'] = lr
        
        try:
            trainer = CS336Trainer(config)
            train_losses, val_losses = trainer.train()
            
            final_val_loss = val_losses[-1] if val_losses else float('inf')
            results[lr] = {
                'train_losses': train_losses,
                'val_losses': val_losses,
                'final_val_loss': final_val_loss,
                'status': 'æ­£å¸¸'
            }
            
            print(f"âœ… LR={lr:.2e} å®Œæˆ | æœ€ç»ˆæŸå¤±: {final_val_loss:.4f}")
            
        except Exception as e:
            print(f"âŒ LR={lr:.2e} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            results[lr] = {
                'train_losses': [], 'val_losses': [], 
                'final_val_loss': float('inf'), 'status': 'å¤±è´¥'
            }
    
    return results

if __name__ == "__main__":
    results = run_lr_sweep_experiment()
    
    # åˆ†æç»“æœ
    if results:
        best_lr = None
        best_loss = float('inf')
        
        for lr, data in sorted(results.items()):
            status = "âœ…æ­£å¸¸" if data['status'] == 'æ­£å¸¸' else "âŒå¤±è´¥"
            loss_str = f"{data['final_val_loss']:.4f}" if data['final_val_loss'] < float('inf') else "å¤±è´¥"
            print(f"LR={lr:.2e}: {status}, æœ€ç»ˆæŸå¤±={loss_str}")
            
            if data['status'] == 'æ­£å¸¸' and data['final_val_loss'] < best_loss:
                best_lr = lr
                best_loss = data['final_val_loss']
        
        if best_lr:
            print(f"\nğŸ† æœ€ä½³ç»“æœ:")
            print(f"   å­¦ä¹ ç‡: {best_lr:.2e}")
            print(f"   éªŒè¯æŸå¤±: {best_loss:.4f}")
            print(f"   æ˜¯å¦è¾¾æ ‡: {'âœ…æ˜¯' if best_loss <= 2.00 else 'âŒå¦'}")